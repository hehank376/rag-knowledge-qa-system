#!/usr/bin/env python3
"""
ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½æ¼”ç¤ºè„šæœ¬

æ¼”ç¤ºç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨çš„åŠŸèƒ½ï¼š
- æ¨¡å‹é…ç½®ç®¡ç†
- æ¨¡å‹åŠ è½½å’Œåˆ‡æ¢
- å¥åº·æ£€æŸ¥å’Œç›‘æ§
- ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
"""

import asyncio
import json
from datetime import datetime
from unittest.mock import AsyncMock, Mock

from rag_system.services.model_manager import (
    ModelManager, 
    ModelConfig, 
    ModelType,
    initialize_global_model_manager,
    get_model_manager,
    cleanup_global_model_manager
)


async def create_demo_model_manager():
    """åˆ›å»ºæ¼”ç¤ºç”¨çš„æ¨¡å‹ç®¡ç†å™¨"""
    config = {
        'auto_load_models': False,  # æ‰‹åŠ¨æ§åˆ¶åŠ è½½
        'enable_model_switching': True,
        'model_cache_size': 3,
        'health_check_interval': 60,
        'default_embedding_models': [
            {
                'name': 'openai_embedding',
                'provider': 'openai',
                'model_name': 'text-embedding-ada-002',
                'config': {
                    'api_key': 'demo_key',
                    'batch_size': 100,
                    'max_tokens': 8192,
                    'timeout': 30
                },
                'enabled': True,
                'priority': 10
            },
            {
                'name': 'local_embedding',
                'provider': 'sentence_transformers',
                'model_name': 'all-MiniLM-L6-v2',
                'config': {
                    'device': 'cpu',
                    'batch_size': 64
                },
                'enabled': True,
                'priority': 8
            },
            {
                'name': 'fallback_embedding',
                'provider': 'mock',
                'model_name': 'mock-embedding',
                'config': {
                    'dimensions': 768,
                    'batch_size': 100
                },
                'enabled': True,
                'priority': 1
            }
        ],
        'default_reranking_models': [
            {
                'name': 'ms_marco_reranking',
                'provider': 'sentence_transformers',
                'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
                'config': {
                    'max_length': 512,
                    'batch_size': 32,
                    'timeout': 30.0,
                    'device': 'cpu'
                },
                'enabled': True,
                'priority': 10
            },
            {
                'name': 'fast_reranking',
                'provider': 'sentence_transformers',
                'model_name': 'cross-encoder/ms-marco-TinyBERT-L-2-v2',
                'config': {
                    'max_length': 256,
                    'batch_size': 64,
                    'timeout': 15.0,
                    'device': 'cpu'
                },
                'enabled': True,
                'priority': 5
            }
        ]
    }
    
    manager = ModelManager(config)
    
    # æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½æ–¹æ³•
    async def mock_load_embedding_model(model_name, config):
        """æ¨¡æ‹ŸåŠ è½½embeddingæ¨¡å‹"""
        print(f"  ğŸ”„ æ¨¡æ‹ŸåŠ è½½embeddingæ¨¡å‹: {model_name} ({config.model_name})")
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
        
        # åˆ›å»ºæ¨¡æ‹ŸæœåŠ¡
        mock_service = AsyncMock()
        mock_service.test_embedding.return_value = {
            'success': True,
            'embedding_dimension': 768,
            'processing_time': 0.05
        }
        mock_service.get_service_stats.return_value = {
            'provider': config.provider,
            'model': config.model_name,
            'dimension': 768,
            'status': 'available'
        }
        
        manager.embedding_services[model_name] = mock_service
        return True
    
    async def mock_load_reranking_model(model_name, config):
        """æ¨¡æ‹ŸåŠ è½½é‡æ’åºæ¨¡å‹"""
        print(f"  ğŸ”„ æ¨¡æ‹ŸåŠ è½½é‡æ’åºæ¨¡å‹: {model_name} ({config.model_name})")
        await asyncio.sleep(0.2)  # æ¨¡æ‹ŸåŠ è½½æ—¶é—´
        
        # åˆ›å»ºæ¨¡æ‹ŸæœåŠ¡
        mock_service = Mock()
        mock_service.health_check = AsyncMock(return_value={
            'status': 'healthy',
            'model_loaded': True
        })
        mock_service.get_metrics.return_value = {
            'model_loaded': True,
            'model_name': config.model_name,
            'total_requests': 0,
            'successful_requests': 0,
            'success_rate': 0.0
        }
        mock_service.close = AsyncMock()
        
        manager.reranking_services[model_name] = mock_service
        return True
    
    # æ›¿æ¢åŠ è½½æ–¹æ³•
    manager._load_embedding_model = mock_load_embedding_model
    manager._load_reranking_model = mock_load_reranking_model
    
    return manager


async def demonstrate_model_management():
    """æ¼”ç¤ºæ¨¡å‹ç®¡ç†åŠŸèƒ½"""
    print("ğŸš€ ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    manager = await create_demo_model_manager()
    await manager.initialize()
    
    print(f"ğŸ“‹ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"   - æ€»æ¨¡å‹æ•°: {manager.stats['total_models']}")
    print(f"   - è‡ªåŠ¨åŠ è½½: {'å¯ç”¨' if manager.auto_load_models else 'ç¦ç”¨'}")
    print(f"   - æ¨¡å‹åˆ‡æ¢: {'å¯ç”¨' if manager.enable_model_switching else 'ç¦ç”¨'}")
    print()
    
    # 1. æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹é…ç½®
    print("1ï¸âƒ£ æ¨¡å‹é…ç½®æ¦‚è§ˆ:")
    print("-" * 40)
    
    configs = manager.get_model_configs()
    embedding_configs = {k: v for k, v in configs.items() if v.model_type == ModelType.EMBEDDING}
    reranking_configs = {k: v for k, v in configs.items() if v.model_type == ModelType.RERANKING}
    
    print(f"ğŸ“Š Embeddingæ¨¡å‹ ({len(embedding_configs)}ä¸ª):")
    for name, config in embedding_configs.items():
        status = "âœ… å¯ç”¨" if config.enabled else "âŒ ç¦ç”¨"
        print(f"   - {name}: {config.provider}/{config.model_name} (ä¼˜å…ˆçº§: {config.priority}) {status}")
    
    print(f"\nğŸ”„ é‡æ’åºæ¨¡å‹ ({len(reranking_configs)}ä¸ª):")
    for name, config in reranking_configs.items():
        status = "âœ… å¯ç”¨" if config.enabled else "âŒ ç¦ç”¨"
        print(f"   - {name}: {config.provider}/{config.model_name} (ä¼˜å…ˆçº§: {config.priority}) {status}")
    print()
    
    # 2. åŠ è½½æ¨¡å‹
    print("2ï¸âƒ£ æ¨¡å‹åŠ è½½æ¼”ç¤º:")
    print("-" * 40)
    
    # åŠ è½½é«˜ä¼˜å…ˆçº§çš„embeddingæ¨¡å‹
    print("ğŸ”„ åŠ è½½embeddingæ¨¡å‹...")
    embedding_models = ['openai_embedding', 'local_embedding', 'fallback_embedding']
    for model_name in embedding_models:
        success = await manager.load_model(model_name)
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   - {model_name}: {status}")
    
    print("\nğŸ”„ åŠ è½½é‡æ’åºæ¨¡å‹...")
    reranking_models = ['ms_marco_reranking', 'fast_reranking']
    for model_name in reranking_models:
        success = await manager.load_model(model_name)
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   - {model_name}: {status}")
    print()
    
    # 3. æ¨¡å‹çŠ¶æ€æ£€æŸ¥
    print("3ï¸âƒ£ æ¨¡å‹çŠ¶æ€æ£€æŸ¥:")
    print("-" * 40)
    
    all_statuses = manager.get_all_model_statuses()
    for name, status in all_statuses.items():
        health_icon = {"healthy": "ğŸ’š", "unhealthy": "â¤ï¸", "unknown": "ğŸ’›"}.get(status.health, "â“")
        status_icon = {"ready": "âœ…", "loading": "ğŸ”„", "error": "âŒ", "unloaded": "â¹ï¸"}.get(status.status, "â“")
        load_time = f" ({status.load_time:.2f}s)" if status.load_time else ""
        print(f"   {status_icon} {name}: {status.status} {health_icon} {status.health}{load_time}")
    print()
    
    # 4. è®¾ç½®æ´»è·ƒæ¨¡å‹
    print("4ï¸âƒ£ è®¾ç½®æ´»è·ƒæ¨¡å‹:")
    print("-" * 40)
    
    # è®¾ç½®æ´»è·ƒembeddingæ¨¡å‹
    success = await manager.switch_active_model(ModelType.EMBEDDING, 'openai_embedding')
    print(f"ğŸ¯ è®¾ç½®æ´»è·ƒembeddingæ¨¡å‹: openai_embedding {'âœ…' if success else 'âŒ'}")
    
    # è®¾ç½®æ´»è·ƒé‡æ’åºæ¨¡å‹
    success = await manager.switch_active_model(ModelType.RERANKING, 'ms_marco_reranking')
    print(f"ğŸ¯ è®¾ç½®æ´»è·ƒé‡æ’åºæ¨¡å‹: ms_marco_reranking {'âœ…' if success else 'âŒ'}")
    print()
    
    # 5. è·å–æ´»è·ƒæœåŠ¡
    print("5ï¸âƒ£ æ´»è·ƒæœåŠ¡æµ‹è¯•:")
    print("-" * 40)
    
    embedding_service = manager.get_active_embedding_service()
    reranking_service = manager.get_active_reranking_service()
    
    print(f"ğŸ“Š æ´»è·ƒembeddingæœåŠ¡: {'âœ… å¯ç”¨' if embedding_service else 'âŒ ä¸å¯ç”¨'}")
    if embedding_service:
        print(f"   - æœåŠ¡ç±»å‹: EmbeddingService")
        print(f"   - è¯·æ±‚ç»Ÿè®¡: {manager.stats['embedding_requests']} æ¬¡")
    
    print(f"ğŸ”„ æ´»è·ƒé‡æ’åºæœåŠ¡: {'âœ… å¯ç”¨' if reranking_service else 'âŒ ä¸å¯ç”¨'}")
    if reranking_service:
        print(f"   - æœåŠ¡ç±»å‹: RerankingService")
        print(f"   - è¯·æ±‚ç»Ÿè®¡: {manager.stats['reranking_requests']} æ¬¡")
    print()
    
    # 6. æ¨¡å‹åˆ‡æ¢æ¼”ç¤º
    print("6ï¸âƒ£ æ¨¡å‹åˆ‡æ¢æ¼”ç¤º:")
    print("-" * 40)
    
    print("ğŸ”„ åˆ‡æ¢åˆ°æœ¬åœ°embeddingæ¨¡å‹...")
    success = await manager.switch_active_model(ModelType.EMBEDDING, 'local_embedding')
    print(f"   ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
    print(f"   å½“å‰æ´»è·ƒæ¨¡å‹: {manager.active_embedding_model}")
    
    print("ğŸ”„ åˆ‡æ¢åˆ°å¿«é€Ÿé‡æ’åºæ¨¡å‹...")
    success = await manager.switch_active_model(ModelType.RERANKING, 'fast_reranking')
    print(f"   ç»“æœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
    print(f"   å½“å‰æ´»è·ƒæ¨¡å‹: {manager.active_reranking_model}")
    print()
    
    # 7. å¥åº·æ£€æŸ¥
    print("7ï¸âƒ£ å¥åº·æ£€æŸ¥:")
    print("-" * 40)
    
    print("ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥...")
    await manager._perform_health_checks()
    
    healthy_count = sum(1 for status in all_statuses.values() if status.health == 'healthy')
    total_count = len(all_statuses)
    
    print(f"ğŸ“Š å¥åº·æ£€æŸ¥ç»“æœ: {healthy_count}/{total_count} ä¸ªæ¨¡å‹å¥åº·")
    
    for name, status in all_statuses.items():
        if status.health == 'unhealthy' and status.error_message:
            print(f"   âš ï¸ {name}: {status.error_message}")
    print()
    
    # 8. ç®¡ç†å™¨ç»Ÿè®¡
    print("8ï¸âƒ£ ç®¡ç†å™¨ç»Ÿè®¡:")
    print("-" * 40)
    
    stats = manager.get_manager_stats()
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ¨¡å‹æ•°: {stats['total_models']}")
    print(f"   - å·²åŠ è½½æ¨¡å‹: {stats['loaded_models']}")
    print(f"   - å¤±è´¥æ¨¡å‹: {stats['failed_models']}")
    print(f"   - æ¨¡å‹åˆ‡æ¢æ¬¡æ•°: {stats['model_switches']}")
    print(f"   - æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"   - Embeddingè¯·æ±‚: {stats['embedding_requests']}")
    print(f"   - é‡æ’åºè¯·æ±‚: {stats['reranking_requests']}")
    print()
    
    print(f"ğŸ¯ æ´»è·ƒæ¨¡å‹:")
    print(f"   - Embedding: {stats['active_embedding_model']}")
    print(f"   - é‡æ’åº: {stats['active_reranking_model']}")
    print()
    
    # 9. æ³¨å†Œæ–°æ¨¡å‹
    print("9ï¸âƒ£ åŠ¨æ€æ³¨å†Œæ–°æ¨¡å‹:")
    print("-" * 40)
    
    new_config = ModelConfig(
        model_type=ModelType.EMBEDDING,
        name="custom_embedding",
        provider="huggingface",
        model_name="sentence-transformers/all-mpnet-base-v2",
        config={
            "device": "cpu",
            "batch_size": 32,
            "normalize_embeddings": True
        },
        enabled=True,
        priority=7
    )
    
    success = await manager.register_model(new_config)
    print(f"ğŸ“ æ³¨å†Œæ–°æ¨¡å‹: custom_embedding {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")
    
    if success:
        print(f"   - æ¨¡å‹ç±»å‹: {new_config.model_type.value}")
        print(f"   - æä¾›å•†: {new_config.provider}")
        print(f"   - æ¨¡å‹å: {new_config.model_name}")
        print(f"   - ä¼˜å…ˆçº§: {new_config.priority}")
        
        # å°è¯•åŠ è½½æ–°æ¨¡å‹
        load_success = await manager.load_model("custom_embedding")
        print(f"   - åŠ è½½ç»“æœ: {'âœ… æˆåŠŸ' if load_success else 'âŒ å¤±è´¥'}")
    print()
    
    # 10. ç»¼åˆçŠ¶æ€æŠ¥å‘Š
    print("ğŸ”Ÿ ç»¼åˆçŠ¶æ€æŠ¥å‘Š:")
    print("-" * 40)
    
    comprehensive_status = await manager.get_comprehensive_status()
    
    print(f"ğŸ“Š ç³»ç»Ÿæ¦‚è§ˆ:")
    manager_stats = comprehensive_status['manager_stats']
    print(f"   - æ¨¡å‹æ€»æ•°: {manager_stats['total_models']}")
    print(f"   - EmbeddingæœåŠ¡: {manager_stats['total_embedding_services']}")
    print(f"   - é‡æ’åºæœåŠ¡: {manager_stats['total_reranking_services']}")
    
    active_models = comprehensive_status['active_models']
    print(f"   - æ´»è·ƒEmbedding: {active_models['embedding']}")
    print(f"   - æ´»è·ƒé‡æ’åº: {active_models['reranking']}")
    print()
    
    # 11. èµ„æºæ¸…ç†
    print("1ï¸âƒ£1ï¸âƒ£ èµ„æºæ¸…ç†:")
    print("-" * 40)
    
    print("ğŸ§¹ å¼€å§‹æ¸…ç†æ¨¡å‹ç®¡ç†å™¨èµ„æº...")
    await manager.cleanup()
    
    print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    print(f"   - EmbeddingæœåŠ¡: {len(manager.embedding_services)} ä¸ª")
    print(f"   - é‡æ’åºæœåŠ¡: {len(manager.reranking_services)} ä¸ª")
    print(f"   - æ¨¡å‹çŠ¶æ€: {len(manager.model_statuses)} ä¸ª")
    print()
    
    print("âœ… ç»Ÿä¸€æ¨¡å‹ç®¡ç†å™¨åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


async def demonstrate_global_model_manager():
    """æ¼”ç¤ºå…¨å±€æ¨¡å‹ç®¡ç†å™¨"""
    print("\nğŸŒ å…¨å±€æ¨¡å‹ç®¡ç†å™¨æ¼”ç¤º:")
    print("-" * 40)
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„å…¨å±€å®ä¾‹
    await cleanup_global_model_manager()
    
    # åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨
    config = {
        'auto_load_models': False,
        'default_embedding_models': [
            {
                'name': 'global_embedding',
                'provider': 'mock',
                'model_name': 'global-mock-embedding',
                'config': {'dimensions': 512},
                'enabled': True,
                'priority': 10
            }
        ]
    }
    
    print("ğŸ”„ åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨...")
    global_manager = await initialize_global_model_manager(config)
    
    print(f"âœ… å…¨å±€ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"   - ç®¡ç†å™¨å®ä¾‹: {type(global_manager).__name__}")
    print(f"   - æ¨¡å‹é…ç½®æ•°: {len(global_manager.model_configs)}")
    
    # è·å–å…¨å±€ç®¡ç†å™¨
    retrieved_manager = get_model_manager()
    print(f"ğŸ” è·å–å…¨å±€ç®¡ç†å™¨: {'âœ… æˆåŠŸ' if retrieved_manager is global_manager else 'âŒ å¤±è´¥'}")
    
    # æ¸…ç†å…¨å±€ç®¡ç†å™¨
    print("ğŸ§¹ æ¸…ç†å…¨å±€æ¨¡å‹ç®¡ç†å™¨...")
    await cleanup_global_model_manager()
    
    final_manager = get_model_manager()
    print(f"âœ… æ¸…ç†å®Œæˆ: {'âœ… æˆåŠŸ' if final_manager is None else 'âŒ å¤±è´¥'}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await demonstrate_model_management()
        await demonstrate_global_model_manager()
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())