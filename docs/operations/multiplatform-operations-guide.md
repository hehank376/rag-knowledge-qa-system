# å¤šå¹³å°æ¨¡å‹è¿ç»´æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç» RAG ç³»ç»Ÿå¤šå¹³å°æ¨¡å‹æ”¯æŒçš„æ—¥å¸¸è¿ç»´ã€ç›‘æ§ã€æ•…éšœå¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

## æ—¥å¸¸è¿ç»´ä»»åŠ¡

### 1. ç³»ç»Ÿå¥åº·æ£€æŸ¥

#### è‡ªåŠ¨åŒ–å¥åº·æ£€æŸ¥

```python
#!/usr/bin/env python3
# daily_health_check.py

import asyncio
import json
import logging
from datetime import datetime
from rag_system.utils.health_checker import ModelHealthChecker

async def daily_health_check():
    """æ¯æ—¥å¥åº·æ£€æŸ¥ä»»åŠ¡"""
    checker = ModelHealthChecker()
    
    print(f"å¼€å§‹å¥åº·æ£€æŸ¥ - {datetime.now()}")
    
    # æ£€æŸ¥æ•´ä½“ç³»ç»Ÿå¥åº·
    health_status = await checker.check_system_health()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "overall_status": health_status["overall_status"],
        "components": {}
    }
    
    for component, status in health_status["components"].items():
        report["components"][component] = {
            "status": status["status"],
            "response_time": status.get("response_time", 0),
            "last_check": status.get("last_check", ""),
            "error": status.get("error", None)
        }
        
        # æ‰“å°ç»„ä»¶çŠ¶æ€
        print(f"{component}: {status['status']}")
        if status["status"] != "healthy":
            print(f"  âš ï¸  é—®é¢˜: {status.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # ä¿å­˜æŠ¥å‘Š
    with open(f"health_reports/health_{datetime.now().strftime('%Y%m%d')}.json", "w") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # å‘é€å‘Šè­¦ï¼ˆå¦‚æœæœ‰é—®é¢˜ï¼‰
    if health_status["overall_status"] != "healthy":
        await send_alert(report)
    
    return report

async def send_alert(report):
    """å‘é€å‘Šè­¦é€šçŸ¥"""
    # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€Slackã€é’‰é’‰ç­‰é€šçŸ¥æ–¹å¼
    print("ğŸš¨ ç³»ç»Ÿå¥åº·æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œéœ€è¦å…³æ³¨ï¼")
    
if __name__ == "__main__":
    asyncio.run(daily_health_check())
```

#### æ‰‹åŠ¨å¥åº·æ£€æŸ¥å‘½ä»¤

```bash
# å¿«é€Ÿå¥åº·æ£€æŸ¥
curl -s http://localhost:8000/api/health | jq '.'

# è¯¦ç»†å¥åº·æ£€æŸ¥
curl -s http://localhost:8000/api/health/detailed | jq '.'

# æ£€æŸ¥ç‰¹å®šç»„ä»¶
curl -s http://localhost:8000/api/health/llm | jq '.'
curl -s http://localhost:8000/api/health/embeddings | jq '.'
```

### 2. æ—¥å¿—ç®¡ç†

#### æ—¥å¿—è½®è½¬é…ç½®

```bash
# /etc/logrotate.d/rag-system
/app/logs/rag-system.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 644 raguser raguser
    postrotate
        systemctl reload rag-system
    endscript
}
```

#### æ—¥å¿—åˆ†æè„šæœ¬

```python
#!/usr/bin/env python3
# log_analyzer.py

import re
import json
from collections import defaultdict, Counter
from datetime import datetime, timedelta

def analyze_logs(log_file, hours=24):
    """åˆ†ææœ€è¿‘Nå°æ—¶çš„æ—¥å¿—"""
    
    cutoff_time = datetime.now() - timedelta(hours=hours)
    
    stats = {
        "total_requests": 0,
        "error_count": 0,
        "response_times": [],
        "providers": Counter(),
        "models": Counter(),
        "error_types": Counter(),
        "slow_requests": []
    }
    
    with open(log_file, 'r') as f:
        for line in f:
            try:
                log_entry = json.loads(line)
                log_time = datetime.fromisoformat(log_entry["timestamp"])
                
                if log_time < cutoff_time:
                    continue
                
                # ç»Ÿè®¡è¯·æ±‚
                if "request" in log_entry.get("message", "").lower():
                    stats["total_requests"] += 1
                
                # ç»Ÿè®¡é”™è¯¯
                if log_entry["level"] == "ERROR":
                    stats["error_count"] += 1
                    stats["error_types"][log_entry.get("error_type", "unknown")] += 1
                
                # ç»Ÿè®¡å“åº”æ—¶é—´
                if "response_time" in log_entry:
                    response_time = log_entry["response_time"]
                    stats["response_times"].append(response_time)
                    
                    # è®°å½•æ…¢è¯·æ±‚
                    if response_time > 5.0:
                        stats["slow_requests"].append({
                            "timestamp": log_entry["timestamp"],
                            "response_time": response_time,
                            "endpoint": log_entry.get("endpoint", "unknown")
                        })
                
                # ç»Ÿè®¡æä¾›å•†å’Œæ¨¡å‹ä½¿ç”¨
                if "provider" in log_entry:
                    stats["providers"][log_entry["provider"]] += 1
                if "model" in log_entry:
                    stats["models"][log_entry["model"]] += 1
                    
            except (json.JSONDecodeError, KeyError):
                continue
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if stats["response_times"]:
        stats["avg_response_time"] = sum(stats["response_times"]) / len(stats["response_times"])
        stats["max_response_time"] = max(stats["response_times"])
        stats["min_response_time"] = min(stats["response_times"])
    
    stats["error_rate"] = stats["error_count"] / max(stats["total_requests"], 1)
    
    return stats

def print_log_summary(stats):
    """æ‰“å°æ—¥å¿—æ‘˜è¦"""
    print("ğŸ“Š æ—¥å¿—åˆ†ææ‘˜è¦")
    print("=" * 50)
    print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"é”™è¯¯æ•°é‡: {stats['error_count']}")
    print(f"é”™è¯¯ç‡: {stats['error_rate']:.2%}")
    
    if stats["response_times"]:
        print(f"å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}s")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {stats['max_response_time']:.2f}s")
        print(f"æœ€å°å“åº”æ—¶é—´: {stats['min_response_time']:.2f}s")
    
    print("\nğŸ”§ æä¾›å•†ä½¿ç”¨ç»Ÿè®¡:")
    for provider, count in stats["providers"].most_common():
        print(f"  {provider}: {count}")
    
    print("\nğŸ¤– æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡:")
    for model, count in stats["models"].most_common():
        print(f"  {model}: {count}")
    
    if stats["error_types"]:
        print("\nâŒ é”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in stats["error_types"].most_common():
            print(f"  {error_type}: {count}")
    
    if stats["slow_requests"]:
        print(f"\nğŸŒ æ…¢è¯·æ±‚ (>{5.0}s): {len(stats['slow_requests'])} ä¸ª")
        for req in stats["slow_requests"][:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  {req['timestamp']}: {req['response_time']:.2f}s - {req['endpoint']}")

if __name__ == "__main__":
    import sys
    log_file = sys.argv[1] if len(sys.argv) > 1 else "/app/logs/rag-system.log"
    hours = int(sys.argv[2]) if len(sys.argv) > 2 else 24
    
    stats = analyze_logs(log_file, hours)
    print_log_summary(stats)
```

### 3. é…ç½®ç®¡ç†

#### é…ç½®å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup_config.sh

BACKUP_DIR="/app/backups/config"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp config/production.yaml $BACKUP_DIR/production_$DATE.yaml
cp .env $BACKUP_DIR/env_$DATE.backup

# å¤‡ä»½æ•°æ®åº“é…ç½®
sqlite3 rag_system.db ".backup $BACKUP_DIR/database_$DATE.db"

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
find $BACKUP_DIR -name "*.yaml" -mtime +30 -delete
find $BACKUP_DIR -name "*.backup" -mtime +30 -delete
find $BACKUP_DIR -name "*.db" -mtime +30 -delete

echo "é…ç½®å¤‡ä»½å®Œæˆ: $DATE"
```

#### é…ç½®éªŒè¯è„šæœ¬

```python
#!/usr/bin/env python3
# validate_config.py

from rag_system.config.loader import ConfigLoader
from rag_system.utils.compatibility import check_config_compatibility
import sys

def validate_config_file(config_path):
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    try:
        loader = ConfigLoader()
        config = loader.load_config(config_path)
        
        print(f"âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®: {config_path}")
        
        # æ£€æŸ¥å…¼å®¹æ€§
        issues = check_config_compatibility(config)
        if issues:
            print("âš ï¸  å‘ç°é…ç½®é—®é¢˜:")
            for issue in issues:
                print(f"  - {issue}")
            return False
        else:
            print("âœ… é…ç½®å…¼å®¹æ€§æ£€æŸ¥é€šè¿‡")
            return True
            
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    config_path = sys.argv[1] if len(sys.argv) > 1 else "config/production.yaml"
    
    if validate_config_file(config_path):
        print("ğŸ‰ é…ç½®éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å®‰å…¨éƒ¨ç½²")
        sys.exit(0)
    else:
        print("ğŸ’¥ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·ä¿®å¤åå†éƒ¨ç½²")
        sys.exit(1)
```

## ç›‘æ§å’Œå‘Šè­¦

### 1. Prometheus æŒ‡æ ‡

#### è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('rag_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('rag_request_duration_seconds', 'Request duration')
ACTIVE_CONNECTIONS = Gauge('rag_active_connections', 'Active connections')
MODEL_RESPONSE_TIME = Histogram('rag_model_response_seconds', 'Model response time', ['provider', 'model'])
ERROR_COUNT = Counter('rag_errors_total', 'Total errors', ['error_type', 'provider'])

class MetricsCollector:
    def __init__(self):
        self.start_time = time.time()
    
    def record_request(self, method, endpoint, status, duration):
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.observe(duration)
    
    def record_model_call(self, provider, model, duration):
        MODEL_RESPONSE_TIME.labels(provider=provider, model=model).observe(duration)
    
    def record_error(self, error_type, provider):
        ERROR_COUNT.labels(error_type=error_type, provider=provider).inc()
    
    def update_active_connections(self, count):
        ACTIVE_CONNECTIONS.set(count)

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
def start_metrics_server(port=9090):
    start_http_server(port)
    print(f"Metrics server started on port {port}")
```

### 2. å‘Šè­¦è§„åˆ™

#### Prometheus å‘Šè­¦è§„åˆ™

```yaml
# alerts.yml
groups:
  - name: rag-system
    rules:
      - alert: HighErrorRate
        expr: rate(rag_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "RAGç³»ç»Ÿé”™è¯¯ç‡è¿‡é«˜"
          description: "é”™è¯¯ç‡è¶…è¿‡10%ï¼Œå½“å‰å€¼: {{ $value }}"
      
      - alert: SlowResponse
        expr: histogram_quantile(0.95, rate(rag_request_duration_seconds_bucket[5m])) > 5
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "RAGç³»ç»Ÿå“åº”ç¼“æ…¢"
          description: "95%åˆ†ä½å“åº”æ—¶é—´è¶…è¿‡5ç§’"
      
      - alert: ModelUnavailable
        expr: up{job="rag-system"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "RAGç³»ç»Ÿä¸å¯ç”¨"
          description: "ç³»ç»Ÿå·²ä¸‹çº¿è¶…è¿‡1åˆ†é’Ÿ"
      
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å†…å­˜ä½¿ç”¨è¿‡é«˜"
          description: "å†…å­˜ä½¿ç”¨è¶…è¿‡1GB"
```

### 3. ç›‘æ§ä»ªè¡¨æ¿

#### Grafana ä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "RAG System Operations Dashboard",
    "tags": ["rag", "operations"],
    "panels": [
      {
        "title": "ç³»ç»Ÿæ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {"expr": "up{job=\"rag-system\"}", "legendFormat": "ç³»ç»ŸçŠ¶æ€"},
          {"expr": "rate(rag_requests_total[5m])", "legendFormat": "è¯·æ±‚é€Ÿç‡"},
          {"expr": "rate(rag_errors_total[5m])", "legendFormat": "é”™è¯¯é€Ÿç‡"}
        ]
      },
      {
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "heatmap",
        "targets": [
          {"expr": "rate(rag_request_duration_seconds_bucket[5m])"}
        ]
      },
      {
        "title": "æ¨¡å‹æ€§èƒ½å¯¹æ¯”",
        "type": "graph",
        "targets": [
          {"expr": "histogram_quantile(0.95, rate(rag_model_response_seconds_bucket[5m]))", "legendFormat": "{{provider}}-{{model}}"}
        ]
      },
      {
        "title": "é”™è¯¯ç±»å‹åˆ†å¸ƒ",
        "type": "piechart",
        "targets": [
          {"expr": "sum by (error_type) (rate(rag_errors_total[5m]))"}
        ]
      }
    ]
  }
}
```

## æ€§èƒ½è°ƒä¼˜

### 1. ç³»ç»Ÿæ€§èƒ½ç›‘æ§

```python
#!/usr/bin/env python3
# performance_monitor.py

import psutil
import time
import json
from datetime import datetime

class PerformanceMonitor:
    def __init__(self):
        self.metrics = []
    
    def collect_metrics(self):
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        network = psutil.net_io_counters()
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "cpu": {
                "percent": cpu_percent,
                "count": psutil.cpu_count()
            },
            "memory": {
                "total": memory.total,
                "available": memory.available,
                "percent": memory.percent,
                "used": memory.used
            },
            "disk": {
                "total": disk.total,
                "used": disk.used,
                "free": disk.free,
                "percent": disk.percent
            },
            "network": {
                "bytes_sent": network.bytes_sent,
                "bytes_recv": network.bytes_recv,
                "packets_sent": network.packets_sent,
                "packets_recv": network.packets_recv
            }
        }
        
        return metrics
    
    def monitor_continuous(self, duration_minutes=60, interval_seconds=30):
        """æŒç»­ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
        end_time = time.time() + (duration_minutes * 60)
        
        while time.time() < end_time:
            metrics = self.collect_metrics()
            self.metrics.append(metrics)
            
            # å®æ—¶æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            print(f"CPU: {metrics['cpu']['percent']:.1f}% | "
                  f"å†…å­˜: {metrics['memory']['percent']:.1f}% | "
                  f"ç£ç›˜: {metrics['disk']['percent']:.1f}%")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
            if metrics['cpu']['percent'] > 80:
                print("âš ï¸  CPUä½¿ç”¨ç‡è¿‡é«˜!")
            if metrics['memory']['percent'] > 85:
                print("âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜!")
            if metrics['disk']['percent'] > 90:
                print("âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³!")
            
            time.sleep(interval_seconds)
        
        return self.metrics
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics:
            return None
        
        cpu_values = [m['cpu']['percent'] for m in self.metrics]
        memory_values = [m['memory']['percent'] for m in self.metrics]
        
        report = {
            "monitoring_period": {
                "start": self.metrics[0]['timestamp'],
                "end": self.metrics[-1]['timestamp'],
                "duration_minutes": len(self.metrics) * 0.5  # å‡è®¾30ç§’é—´éš”
            },
            "cpu": {
                "avg": sum(cpu_values) / len(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values)
            },
            "memory": {
                "avg": sum(memory_values) / len(memory_values),
                "max": max(memory_values),
                "min": min(memory_values)
            },
            "recommendations": []
        }
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if report['cpu']['avg'] > 70:
            report['recommendations'].append("è€ƒè™‘å¢åŠ CPUèµ„æºæˆ–ä¼˜åŒ–è®¡ç®—å¯†é›†å‹æ“ä½œ")
        if report['memory']['avg'] > 80:
            report['recommendations'].append("è€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        return report

if __name__ == "__main__":
    monitor = PerformanceMonitor()
    print("å¼€å§‹æ€§èƒ½ç›‘æ§...")
    
    try:
        monitor.monitor_continuous(duration_minutes=10, interval_seconds=30)
        report = monitor.generate_report()
        
        print("\nğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        print("=" * 50)
        print(json.dumps(report, indent=2, ensure_ascii=False))
        
    except KeyboardInterrupt:
        print("\nç›‘æ§å·²åœæ­¢")
```

### 2. æ¨¡å‹æ€§èƒ½ä¼˜åŒ–

```python
# model_optimizer.py

class ModelOptimizer:
    def __init__(self):
        self.optimization_strategies = {
            'batch_size': self.optimize_batch_size,
            'concurrent_requests': self.optimize_concurrency,
            'timeout_settings': self.optimize_timeouts,
            'caching': self.optimize_caching
        }
    
    def optimize_batch_size(self, current_config):
        """ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°"""
        current_batch_size = current_config.get('batch_size', 100)
        
        # åŸºäºå†…å­˜å’Œæ€§èƒ½æµ‹è¯•ç»“æœè°ƒæ•´
        if current_config.get('memory_usage_percent', 0) > 80:
            recommended_batch_size = max(50, current_batch_size // 2)
            return {
                'batch_size': recommended_batch_size,
                'reason': 'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå‡å°‘æ‰¹å¤„ç†å¤§å°'
            }
        elif current_config.get('avg_response_time', 0) > 3.0:
            recommended_batch_size = max(50, current_batch_size // 2)
            return {
                'batch_size': recommended_batch_size,
                'reason': 'å“åº”æ—¶é—´è¿‡é•¿ï¼Œå‡å°‘æ‰¹å¤„ç†å¤§å°'
            }
        elif current_config.get('cpu_usage_percent', 0) < 50:
            recommended_batch_size = min(200, current_batch_size * 1.5)
            return {
                'batch_size': int(recommended_batch_size),
                'reason': 'CPUä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥å¢åŠ æ‰¹å¤„ç†å¤§å°'
            }
        
        return {'batch_size': current_batch_size, 'reason': 'å½“å‰é…ç½®å·²ä¼˜åŒ–'}
    
    def optimize_concurrency(self, current_config):
        """ä¼˜åŒ–å¹¶å‘è¯·æ±‚æ•°"""
        current_concurrency = current_config.get('max_concurrent_requests', 5)
        cpu_count = psutil.cpu_count()
        
        if current_config.get('error_rate', 0) > 0.05:
            recommended_concurrency = max(2, current_concurrency - 1)
            return {
                'max_concurrent_requests': recommended_concurrency,
                'reason': 'é”™è¯¯ç‡è¿‡é«˜ï¼Œå‡å°‘å¹¶å‘æ•°'
            }
        elif current_config.get('cpu_usage_percent', 0) < 60:
            recommended_concurrency = min(cpu_count * 2, current_concurrency + 1)
            return {
                'max_concurrent_requests': recommended_concurrency,
                'reason': 'CPUèµ„æºå……è¶³ï¼Œå¯ä»¥å¢åŠ å¹¶å‘æ•°'
            }
        
        return {'max_concurrent_requests': current_concurrency, 'reason': 'å½“å‰å¹¶å‘é…ç½®å·²ä¼˜åŒ–'}
    
    def generate_optimization_report(self, current_config, performance_metrics):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        config_with_metrics = {**current_config, **performance_metrics}
        
        recommendations = {}
        for strategy_name, strategy_func in self.optimization_strategies.items():
            recommendations[strategy_name] = strategy_func(config_with_metrics)
        
        return {
            'current_config': current_config,
            'performance_metrics': performance_metrics,
            'recommendations': recommendations,
            'timestamp': datetime.now().isoformat()
        }
```

è¿™ä¸ªè¿ç»´æŒ‡å—æä¾›äº†å®Œæ•´çš„æ—¥å¸¸è¿ç»´æµç¨‹ã€ç›‘æ§å‘Šè­¦æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œå¸®åŠ©è¿ç»´å›¢é˜Ÿæœ‰æ•ˆç®¡ç†å¤šå¹³å°æ¨¡å‹ç³»ç»Ÿã€‚