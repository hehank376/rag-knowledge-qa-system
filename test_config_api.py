#!/usr/bin/env python3
"""
æµ‹è¯•é…ç½®APIçš„è„šæœ¬
"""
import requests
import json
import sys

def test_config_api():
    base_url = "http://localhost:8000"
    
    print("ğŸ§ª æµ‹è¯•é…ç½®API...")
    
    # æµ‹è¯•è·å–é…ç½®
    print("\n1. æµ‹è¯•è·å–é…ç½®...")
    try:
        response = requests.get(f"{base_url}/config/")
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            config = response.json()
            print("âœ“ è·å–é…ç½®æˆåŠŸ")
            print(f"é…ç½®å†…å®¹: {json.dumps(config, indent=2, ensure_ascii=False)}")
        else:
            print(f"âœ— è·å–é…ç½®å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"âœ— è·å–é…ç½®å¼‚å¸¸: {e}")
    
    # æµ‹è¯•LLMè¿æ¥æµ‹è¯•
    print("\n2. æµ‹è¯•LLMè¿æ¥...")
    try:
        llm_config = {
            "provider": "mock",
            "model": "test-model",
            "temperature": 0.7,
            "max_tokens": 1000
        }
        response = requests.post(f"{base_url}/config/test/llm", json=llm_config)
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print("âœ“ LLMè¿æ¥æµ‹è¯•æˆåŠŸ")
            print(f"æµ‹è¯•ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        else:
            print(f"âœ— LLMè¿æ¥æµ‹è¯•å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"âœ— LLMè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
    
    # æµ‹è¯•åµŒå…¥æ¨¡å‹è¿æ¥æµ‹è¯•
    print("\n3. æµ‹è¯•åµŒå…¥æ¨¡å‹è¿æ¥...")
    try:
        embedding_config = {
            "provider": "mock",
            "model": "test-embedding",
            "chunk_size": 1000,
            "chunk_overlap": 200
        }
        response = requests.post(f"{base_url}/config/test/embedding", json=embedding_config)
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print("âœ“ åµŒå…¥æ¨¡å‹è¿æ¥æµ‹è¯•æˆåŠŸ")
            print(f"æµ‹è¯•ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        else:
            print(f"âœ— åµŒå…¥æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"âœ— åµŒå…¥æ¨¡å‹è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
    
    # æµ‹è¯•å­˜å‚¨è¿æ¥æµ‹è¯•
    print("\n4. æµ‹è¯•å­˜å‚¨è¿æ¥...")
    try:
        storage_config = {
            "type": "chroma",
            "persist_directory": "./data/chroma",
            "collection_name": "test_collection"
        }
        response = requests.post(f"{base_url}/config/test/storage", json=storage_config)
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print("âœ“ å­˜å‚¨è¿æ¥æµ‹è¯•æˆåŠŸ")
            print(f"æµ‹è¯•ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        else:
            print(f"âœ— å­˜å‚¨è¿æ¥æµ‹è¯•å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"âœ— å­˜å‚¨è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
    
    # æµ‹è¯•é…ç½®æ›´æ–°
    print("\n5. æµ‹è¯•é…ç½®æ›´æ–°...")
    try:
        update_config = {
            "llm": {
                "provider": "mock",
                "model": "updated-model",
                "temperature": 0.8,
                "max_tokens": 1500
            },
            "embeddings": {
                "provider": "mock",
                "model": "updated-embedding",
                "chunk_size": 1200,
                "chunk_overlap": 250
            }
        }
        response = requests.put(f"{base_url}/config/all", json=update_config)
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            print("âœ“ é…ç½®æ›´æ–°æˆåŠŸ")
            print(f"æ›´æ–°ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
        else:
            print(f"âœ— é…ç½®æ›´æ–°å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"âœ— é…ç½®æ›´æ–°å¼‚å¸¸: {e}")
    
    print("\nğŸ‰ é…ç½®APIæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_config_api()